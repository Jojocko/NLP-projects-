{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNbeIX0PYhZN3BXKcK12foy",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jojocko/NLP-projects-/blob/main/Suite_we_cbow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/Datasets NLP/dataset_supplychain.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmZ-Nzej6EP_",
        "outputId": "f5c0f46f-67c0-46e3-9b5c-ed11cd21e0ae"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['client', 'langage', 'reponse'], axis=1)\n",
        "df.rename(columns={'Sentiment': 'sentiment', 'Commentaire': 'commentaire'}, inplace=True)\n",
        "df['sentiment'] = df['sentiment'].replace({'__label__POSITIVE': 'positif', '__label__NEGATIVE': 'negatif', '__label__NEUTRAL': 'neutre'})\n",
        "df['date'] = df['date'].fillna(method=\"ffill\")\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'] = df['date'].dt.year # pour visualiser par année et non par jour/mois/année"
      ],
      "metadata": {
        "id": "PFCpZITa6WAJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('french'))\n",
        "\n",
        "jugement = {'très', 'extrêmement', 'particulièrement', 'exceptionnellement','tout à fait', 'absolument', 'complètement', 'entièrement', 'parfaitement', 'profondément', 'hautement', 'tout', 'plutôt', 'assez', 'bien', 'bon','vraiment', 'totalement', 'énormément', 'peu', 'moins'}\n",
        "\n",
        "satisfaction = {'satisfait', 'content', 'heureux', 'ravi', 'enchanté', 'comblé', 'agréable', 'plaisant', 'positif', 'excellent', 'remarquable', 'exceptionnel', 'superbe', 'admirable', 'réjoui', 'gratifiant', 'récompensant', 'conquis', 'impressionné', 'élogieux'}\n",
        "insatisfaction = {'insatisfait', 'mécontent', 'déçu', 'frustré', 'contrarié', 'désappointé', 'inacceptable', 'problématique', 'inadmissible', 'déplorable', 'lamentable', 'irrité', 'en colère', 'révolté', 'amère', 'négatif', 'critique', 'malheureux', 'peu convaincu', 'regrettable'}\n",
        "company = {'Fnac', 'fnac', 'Amazon', 'amazon', 'CDiscount', 'cdiscount'}\n",
        "\n",
        "stop_words.update(jugement, satisfaction, insatisfaction, company)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd4K5P597rV3",
        "outputId": "524eb17b-a800-46c2-d76f-131210c93dae"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Traitement des données pour clustering des commentaires négatifs\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip()) # lower capitalized letters + retire les espaces en début et fin de string (phrases)\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w) # ajout d'espaces autour des ponctuations pour les séparer des mots. eg: \"he is a boy.\" => \"he is a boy .\n",
        "    w = re.sub(r'[\" \"]+', \" \", w) # remplacement des séquences d'espaces multiples par un seul espace\n",
        "    w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w) # suppression de tout caractère qui n'est pas une lettre ou une ponctuation courante\n",
        "    w = re.sub(r'\\b\\w{0,2}\\b', '', w) # suppression des mots de moins de trois lettres.\n",
        "\n",
        "    # remove stopword\n",
        "    mots = word_tokenize(w.strip()) # tokénization en mots individuels\n",
        "    mots = [mot for mot in mots if mot not in stop_words] # filtrage des mots vides\n",
        "    return ' '.join(mots).strip() # reconstruction de la phrase sans les mots vides\n",
        "\n",
        "df.cleaned_lemma = df.cleaned_lemma.apply(lambda x :preprocess_sentence(x))"
      ],
      "metadata": {
        "id": "9TjZYw5e6bjW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création de l'ensemble de données pour archtecture CBOW\n",
        "\n",
        "# Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000) # doc limité à 10000 mots\n",
        "tokenizer.fit_on_texts(df.cleaned_lemma) # application du dic sur les commentaires\n",
        "\n",
        "word2idx = tokenizer.word_index # stocke le dictionnaire de correspondance entre mots et index\n",
        "idx2word = tokenizer.index_word # stocke le dictionnaire de correspondance entre index et mots\n",
        "vocab_size = tokenizer.num_words # stocke la taille du dictionnaire\n",
        "\n",
        "# Ensemble de données\n",
        "import numpy as np\n",
        "\n",
        "def sentenceToData(tokens, WINDOW_SIZE):\n",
        "    context_size = WINDOW_SIZE // 2\n",
        "    window = np.concatenate((np.arange(-context_size, 0), np.arange(1, context_size + 1)))\n",
        "    X, Y = [], []\n",
        "    for word_index, word in enumerate(tokens):\n",
        "        if (word_index - context_size >= 0) and (word_index + context_size < len(tokens)):\n",
        "            context = [tokens[word_index + offset] for offset in window]\n",
        "            target = word\n",
        "            X.append(context)\n",
        "            Y.append(target)\n",
        "    return X, Y\n",
        "\n",
        "WINDOW_SIZE = 5  # Fenêtre de contexte autour du mot cible\n",
        "\n",
        "X, Y = [], []\n",
        "for review in df['cleaned_lemma'][df['sentiment'] == 'negatif']:\n",
        "    sentences = review.split(\".\")  # transforme les commentaires en phrases\n",
        "    for sentence in sentences:\n",
        "        word_list = tokenizer.texts_to_sequences([sentence.strip()])[0]  # convertit les phrases en séquence\n",
        "        if len(word_list) >= WINDOW_SIZE:\n",
        "            X_temp, Y_temp = sentenceToData(word_list, WINDOW_SIZE)\n",
        "            X.extend(X_temp)\n",
        "            Y.extend(Y_temp)\n",
        "\n",
        "# Convert X and Y to numpy arrays\n",
        "X = np.array(X).astype(int)\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X = pad_sequences(X, padding='post')\n",
        "Y = np.array(Y).reshape(-1, 1)\n",
        "\n",
        "print('Shape of X:', X.shape)\n",
        "print('Shape of Y:', Y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lWAY5ck72pB",
        "outputId": "4d3d79b0-09ad-4dcc-edc6-aeec34d984a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (764323, 4)\n",
            "Shape of Y: (764323, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture CBOW\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Dropout, GRU\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "vocab_size = min(len(word2idx) + 1, tokenizer.num_words + 1)\n",
        "embedding_dim = 300\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(GRU(units=128, return_sequences=False))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T77NUTOc9WF2",
        "outputId": "51cf0b09-801c-4693-b764-3261cfd26501"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 300)         3000300   \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 128)               165120    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               33024     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10001)             2570257   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5768701 (22.01 MB)\n",
            "Trainable params: 5768701 (22.01 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z67u48sa5760",
        "outputId": "42c6cd02-a103-4fb8-d579-e77575e263d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "# Chargement du CBOW\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "model.load_weights('/content/drive/My Drive/Datasets NLP/weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "embeddings = model.layers[0].get_weights()[0]\n",
        "\n",
        "# Fonction pour trouver les mots les plus similaires dans les embeddings\n",
        "def find_most_similar(embeddings, word, word_to_index, index_to_word, n=10):\n",
        "    word_idx = word_to_index[word] # obtenir l'index des mots\n",
        "    word_embedding = embeddings[word_idx] # obtenir l'embedding du mot cible\n",
        "    similarities = cosine_similarity([word_embedding], embeddings)[0] # calcul de similarité entre embedding du mot cible et tous les autres embeddings\n",
        "    most_similar = np.argsort(similarities)[::-1][1:n+1]  # obtenir les indices des mots les plus similaires en ignorant le premier (le mot lui-même)\n",
        "    similar_words = [(index_to_word[i], similarities[i]) for i in most_similar] # obtenir les mots similaires et leurs scores\n",
        "    return similar_words\n",
        "\n",
        "# Correspondance index-mot\n",
        "index_to_word = {index: word for word, index in tokenizer.word_index.items()}"
      ],
      "metadata": {
        "id": "2hu1uNXl_KfY"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création de clusters bi-grammes\n",
        "\n",
        "import numpy as np\n",
        "from nltk import ngrams\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "negative_comments = df[df['sentiment'] == 'negatif']['cleaned_lemma']\n",
        "\n",
        "# Générer un mix d'unigrammes, bigrammes, et trigrammes\n",
        "n_gram_list = []\n",
        "for comment in negative_comments:\n",
        "    words = text_to_word_sequence(comment)  # tokenization\n",
        "    for n in range(1, 4):  # Pour générer unigrammes, bigrammes, et trigrammes\n",
        "        n_grams = list(ngrams(words, n, pad_left=True, pad_right=True, left_pad_symbol='', right_pad_symbol=''))\n",
        "        n_gram_list.extend(n_grams)\n",
        "\n",
        "# Représentation vectorielle pour chaque n-gramme\n",
        "n_gram_vectors = []\n",
        "for n_gram in n_gram_list:\n",
        "    # Filtrer les '' introduits par padding pour calculer l'indice correctement\n",
        "    filtered_n_gram = [word for word in n_gram if word != '']\n",
        "    indices = [tokenizer.word_index.get(word, 0) for word in filtered_n_gram]\n",
        "    # Vérifier si les indices sont valides (< vocab_size)\n",
        "    if all(idx < vocab_size for idx in indices):\n",
        "        word_vectors = np.array([embeddings[idx] for idx in indices])\n",
        "        n_gram_vector = np.mean(word_vectors, axis=0) if len(word_vectors) > 0 else np.zeros(embeddings.shape[1])\n",
        "        n_gram_vectors.append(n_gram_vector)\n",
        "\n",
        "# Standardisation des vecteurs de n-grammes\n",
        "scaler = StandardScaler()\n",
        "n_gram_vectors_scaled = scaler.fit_transform(n_gram_vectors)\n"
      ],
      "metadata": {
        "id": "mWyCsRn16p4A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA pour réduire la dimensionnalité\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.95)  # Conserver 95% de la variance explicative\n",
        "n_gram_vectors_pca = pca.fit_transform(n_gram_vectors_scaled)\n",
        "\n",
        "# Clustering avec KMeans sur les données réduites\n",
        "k = 10  # Nombre de clusters\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(n_gram_vectors_pca)\n",
        "clusters = kmeans.labels_\n",
        "\n",
        "# Affichage des clusters\n",
        "cluster_n_grams = {i: [] for i in range(k)}\n",
        "for i, label in enumerate(clusters):\n",
        "    n_gram = n_gram_list[i]\n",
        "    cluster_n_grams[label].append(\" \".join(n_gram))\n",
        "\n",
        "# Afficher les n-grammes pour chaque cluster\n",
        "for cluster, n_grams in cluster_n_grams.items():\n",
        "    print(f\"Cluster {cluster}:\")\n",
        "    for n_gram in n_grams[:10]:  # Afficher les 10 premiers n-grammes pour concision\n",
        "        print(n_gram)\n",
        "    print(\"\\n---\\n\")"
      ],
      "metadata": {
        "id": "X-SlCdvP6qsG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score de silhouette pour aider à identifier un nombre optimal de clusters pour votre analyse, en fonction du score de cahque cluster\n",
        "\n",
        "# Calcul long d'où échantillonnage des données\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Calculer le score de silhouette avec un échantillon limité\n",
        "score = silhouette_score(n_gram_vectors_pca, kmeans.labels_, sample_size=50000)\n",
        "print(f\"Score de silhouette (limité): {score}\")\n"
      ],
      "metadata": {
        "id": "I87-X-bjAr9S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test des clusters formés\n",
        "\n",
        "ref_df = pd.read_csv('/content/drive/My Drive/Datasets NLP/ecommerce_plaintes.csv')\n",
        "\n",
        "# Accès aux commentaires\n",
        "reference_sentences = ref_df['commentaire'].tolist()\n",
        "\n",
        "# Tokenisation et conversion en embeddings pour les phrases de référence\n",
        "ref_n_gram_vectors = []\n",
        "\n",
        "for sentence in reference_sentences:\n",
        "    words = text_to_word_sequence(sentence)\n",
        "    n_grams = list(ngrams(words, n))  # n étant bigramme\n",
        "    sentence_vectors = []\n",
        "    for n_gram in n_grams:\n",
        "        indices = [tokenizer.word_index.get(word, 0) for word in n_gram]\n",
        "        if all(idx < vocab_size for idx in indices):\n",
        "            word_vectors = embeddings[indices]\n",
        "            n_gram_vector = np.mean(word_vectors, axis=0)\n",
        "            sentence_vectors.append(n_gram_vector)\n",
        "    # Moyenne des embeddings des n-grammes pour obtenir un vecteur par phrase\n",
        "    if sentence_vectors:\n",
        "        ref_n_gram_vectors.append(np.mean(sentence_vectors, axis=0))\n",
        "\n",
        "ref_n_gram_vectors = np.array(ref_n_gram_vectors)\n"
      ],
      "metadata": {
        "id": "DUsJ1xL4AvWg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Même objet PCA que pour les données d'entraînement\n",
        "\n",
        "ref_n_gram_vectors_pca = pca.transform(ref_n_gram_vectors)"
      ],
      "metadata": {
        "id": "UyEmCNqxAxsD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de similarité\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Calculer la similarité de chaque phrase de référence avec les centroïdes des clusters\n",
        "similarities = cosine_similarity(ref_n_gram_vectors_pca, kmeans.cluster_centers_)\n",
        "\n",
        "# Pour chaque phrase, identification du cluster le plus proche\n",
        "closest_clusters = np.argmax(similarities, axis=1)\n",
        "\n",
        "for i, cluster_num in enumerate(closest_clusters):\n",
        "    print(f\"Phrase '{reference_sentences[i]}' est la plus proche du cluster {cluster_num}\")\n"
      ],
      "metadata": {
        "id": "7rL2hBFvAz-c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from joblib import dump, load\n",
        "\n",
        "# Sauvegarder le modèle\n",
        "dump(kmeans, 'kmeans_model.joblib')\n",
        "\n",
        "# Sauvegarder les vecteurs d'embedding\n",
        "dump(n_gram_vectors_scaled, 'n_gram_vectors_scaled.joblib')"
      ],
      "metadata": {
        "id": "b8Bj-FS-A3BF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Later charger les modèles:\n",
        "\n",
        "# Kmeans :\n",
        "# kmeans = load('kmeans_model.joblib')\n",
        "\n",
        "# Vecteurs d'embedding\n",
        "# n_gram_vectors_scaled = load('n_gram_vectors_scaled.joblib')\n",
        "\n",
        "# Utiliser le modèle chargé pour prédire les clusters d'un nouvel ensemble de données, ou réutiliser les labels déjà calculés\n",
        "# clusters = kmeans.labels_\n",
        "# ou kmeans.predict(nouvel_ensemble_de_données_mis_à_l'échelle)"
      ],
      "metadata": {
        "id": "76PZxAqWA3fA"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}