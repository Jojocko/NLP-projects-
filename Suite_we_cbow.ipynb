{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V100",
      "authorship_tag": "ABX9TyPMcd2S4l1OV3AgjxazWYuw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jojocko/NLP-projects-/blob/main/Suite_we_cbow.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/Datasets NLP/dataset_supplychain.csv')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmZ-Nzej6EP_",
        "outputId": "5cfd94b4-2440-41eb-a1dd-4cbae84de99f"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = df.drop(['client', 'langage', 'reponse'], axis=1)\n",
        "df.rename(columns={'Sentiment': 'sentiment', 'Commentaire': 'commentaire'}, inplace=True)\n",
        "df['sentiment'] = df['sentiment'].replace({'__label__POSITIVE': 'positif', '__label__NEGATIVE': 'negatif', '__label__NEUTRAL': 'neutre'})\n",
        "df['date'] = df['date'].fillna(method=\"ffill\")\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'] = df['date'].dt.year # pour visualiser par année et non par jour/mois/année"
      ],
      "metadata": {
        "id": "PFCpZITa6WAJ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('french'))\n",
        "\n",
        "jugement = {'très', 'extrêmement', 'particulièrement', 'exceptionnellement','tout à fait', 'absolument', 'complètement', 'entièrement', 'parfaitement', 'profondément', 'hautement', 'tout', 'plutôt', 'assez', 'bien', 'bon','vraiment', 'totalement', 'énormément', 'peu', 'moins'}\n",
        "satisfaction = {'satisfait', 'content', 'heureux', 'ravi', 'enchanté', 'comblé', 'agréable', 'plaisant', 'positif', 'excellent', 'remarquable', 'exceptionnel', 'superbe', 'admirable', 'réjoui', 'gratifiant', 'récompensant', 'conquis', 'impressionné', 'élogieux'}\n",
        "insatisfaction = {'insatisfait', 'mécontent', 'déçu', 'frustré', 'contrarié', 'désappointé', 'inacceptable', 'problématique', 'inadmissible', 'déplorable', 'lamentable', 'irrité', 'en colère', 'révolté', 'amère', 'négatif', 'critique', 'malheureux', 'peu convaincu', 'regrettable'}\n",
        "company = {'Fnac', 'fnac', 'Amazon', 'amazon', 'CDiscount', 'cdiscount'}\n",
        "\n",
        "stop_words.update(jugement, satisfaction, insatisfaction, company)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd4K5P597rV3",
        "outputId": "f5be7d35-5553-4c6c-86d6-d18867fc77fb"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Traitement des données pour clustering des commentaires négatifs\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip()) # lower capitalized letters + retire les espaces en début et fin de string (phrases)\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w) # ajout d'espaces autour des ponctuations pour les séparer des mots. eg: \"he is a boy.\" => \"he is a boy .\n",
        "    w = re.sub(r'[\" \"]+', \" \", w) # remplacement des séquences d'espaces multiples par un seul espace\n",
        "    w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w) # suppression de tout caractère qui n'est pas une lettre ou une ponctuation courante\n",
        "    w = re.sub(r'\\b\\w{0,2}\\b', '', w) # suppression des mots de moins de trois lettres.\n",
        "\n",
        "    # remove stopword\n",
        "    mots = word_tokenize(w.strip()) # tokénization en mots individuels\n",
        "    mots = [mot for mot in mots if mot not in stop_words] # filtrage des mots vides\n",
        "    return ' '.join(mots).strip() # reconstruction de la phrase sans les mots vides\n",
        "\n",
        "df.cleaned_lemma = df.cleaned_lemma.apply(lambda x :preprocess_sentence(x))"
      ],
      "metadata": {
        "id": "9TjZYw5e6bjW"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Création de l'ensemble de données pour archtecture CBOW\n",
        "\n",
        "# Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000) # doc limité à 10000 mots\n",
        "tokenizer.fit_on_texts(df.cleaned_lemma) # application du dic sur les commentaires\n",
        "\n",
        "word2idx = tokenizer.word_index # stocke le dictionnaire de correspondance entre mots et index\n",
        "idx2word = tokenizer.index_word # stocke le dictionnaire de correspondance entre index et mots\n",
        "vocab_size = tokenizer.num_words # stocke la taille du dictionnaire\n",
        "\n",
        "# Ensemble de données\n",
        "import numpy as np\n",
        "\n",
        "def sentenceToData(tokens, WINDOW_SIZE):\n",
        "    context_size = WINDOW_SIZE // 2\n",
        "    window = np.concatenate((np.arange(-context_size, 0), np.arange(1, context_size + 1)))\n",
        "    X, Y = [], []\n",
        "    for word_index, word in enumerate(tokens):\n",
        "        if (word_index - context_size >= 0) and (word_index + context_size < len(tokens)):\n",
        "            context = [tokens[word_index + offset] for offset in window]\n",
        "            target = word\n",
        "            X.append(context)\n",
        "            Y.append(target)\n",
        "    return X, Y\n",
        "\n",
        "WINDOW_SIZE = 5  # Fenêtre de contexte autour du mot cible\n",
        "\n",
        "X, Y = [], []\n",
        "for review in df['cleaned_lemma'][df['sentiment'] == 'negatif']:\n",
        "    sentences = review.split(\".\")  # transforme les commentaires en phrases\n",
        "    for sentence in sentences:\n",
        "        word_list = tokenizer.texts_to_sequences([sentence.strip()])[0]  # convertit les phrases en séquence\n",
        "        if len(word_list) >= WINDOW_SIZE:\n",
        "            X_temp, Y_temp = sentenceToData(word_list, WINDOW_SIZE)\n",
        "            X.extend(X_temp)\n",
        "            Y.extend(Y_temp)\n",
        "\n",
        "# Convert X and Y to numpy arrays\n",
        "X = np.array(X).astype(int)\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X = pad_sequences(X, padding='post')\n",
        "Y = np.array(Y).reshape(-1, 1)\n",
        "\n",
        "print('Shape of X:', X.shape)\n",
        "print('Shape of Y:', Y.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lWAY5ck72pB",
        "outputId": "8bf0b79a-bac6-4209-c5c1-d848fd06db3f"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (764323, 4)\n",
            "Shape of Y: (764323, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Architecture CBOW\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Dropout, GRU\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "vocab_size = min(len(word2idx) + 1, tokenizer.num_words + 1)\n",
        "embedding_dim = 512\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "embedding_layer.trainable = False\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(GRU(units=128, return_sequences=False))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "model.fit(X, Y, epochs=20, batch_size=64, callbacks=[early_stopping], validation_split=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T77NUTOc9WF2",
        "outputId": "9b3c5964-d86b-41cc-fb59-e4ebcf36c8d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 512)         5120512   \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 128)               246528    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               33024     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10001)             2570257   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7970321 (30.40 MB)\n",
            "Trainable params: 2849809 (10.87 MB)\n",
            "Non-trainable params: 5120512 (19.53 MB)\n",
            "_________________________________________________________________\n",
            "Epoch 1/20\n",
            "10749/10749 [==============================] - 70s 6ms/step - loss: 6.8140 - accuracy: 0.0501 - val_loss: 6.6742 - val_accuracy: 0.0665\n",
            "Epoch 2/20\n",
            "10749/10749 [==============================] - 67s 6ms/step - loss: 6.4799 - accuracy: 0.0807 - val_loss: 6.5263 - val_accuracy: 0.0822\n",
            "Epoch 3/20\n",
            "10749/10749 [==============================] - 66s 6ms/step - loss: 6.3117 - accuracy: 0.0922 - val_loss: 6.4505 - val_accuracy: 0.0901\n",
            "Epoch 4/20\n",
            "10749/10749 [==============================] - 66s 6ms/step - loss: 6.2111 - accuracy: 0.0984 - val_loss: 6.4143 - val_accuracy: 0.0955\n",
            "Epoch 5/20\n",
            "10749/10749 [==============================] - 66s 6ms/step - loss: 6.1414 - accuracy: 0.1020 - val_loss: 6.3834 - val_accuracy: 0.0993\n",
            "Epoch 6/20\n",
            "10749/10749 [==============================] - 67s 6ms/step - loss: 6.0880 - accuracy: 0.1052 - val_loss: 6.3706 - val_accuracy: 0.1024\n",
            "Epoch 7/20\n",
            "10749/10749 [==============================] - 66s 6ms/step - loss: 6.0498 - accuracy: 0.1077 - val_loss: 6.3513 - val_accuracy: 0.1038\n",
            "Epoch 8/20\n",
            "10749/10749 [==============================] - 66s 6ms/step - loss: 6.0152 - accuracy: 0.1095 - val_loss: 6.3373 - val_accuracy: 0.1061\n",
            "Epoch 9/20\n",
            "10749/10749 [==============================] - 66s 6ms/step - loss: 5.9874 - accuracy: 0.1115 - val_loss: 6.3278 - val_accuracy: 0.1065\n",
            "Epoch 10/20\n",
            "10749/10749 [==============================] - 66s 6ms/step - loss: 5.9668 - accuracy: 0.1124 - val_loss: 6.3272 - val_accuracy: 0.1071\n",
            "Epoch 11/20\n",
            "10749/10749 [==============================] - 67s 6ms/step - loss: 5.9475 - accuracy: 0.1135 - val_loss: 6.3346 - val_accuracy: 0.1112\n",
            "Epoch 12/20\n",
            "10749/10749 [==============================] - 67s 6ms/step - loss: 5.9292 - accuracy: 0.1150 - val_loss: 6.3079 - val_accuracy: 0.1106\n",
            "Epoch 13/20\n",
            "10749/10749 [==============================] - 67s 6ms/step - loss: 5.9143 - accuracy: 0.1157 - val_loss: 6.3098 - val_accuracy: 0.1118\n",
            "Epoch 14/20\n",
            "10749/10749 [==============================] - 67s 6ms/step - loss: 5.9016 - accuracy: 0.1166 - val_loss: 6.2849 - val_accuracy: 0.1123\n",
            "Epoch 15/20\n",
            "10749/10749 [==============================] - 65s 6ms/step - loss: 5.8882 - accuracy: 0.1175 - val_loss: 6.3118 - val_accuracy: 0.1129\n",
            "Epoch 16/20\n",
            "10749/10749 [==============================] - 67s 6ms/step - loss: 5.8778 - accuracy: 0.1186 - val_loss: 6.2787 - val_accuracy: 0.1132\n",
            "Epoch 17/20\n",
            "10749/10749 [==============================] - 66s 6ms/step - loss: 5.8658 - accuracy: 0.1192 - val_loss: 6.2836 - val_accuracy: 0.1146\n",
            "Epoch 18/20\n",
            "10749/10749 [==============================] - 67s 6ms/step - loss: 5.8558 - accuracy: 0.1197 - val_loss: 6.2956 - val_accuracy: 0.1152\n",
            "Epoch 19/20\n",
            "10749/10749 [==============================] - 66s 6ms/step - loss: 5.8501 - accuracy: 0.1203 - val_loss: 6.2820 - val_accuracy: 0.1153\n",
            "Epoch 20/20\n",
            "10749/10749 [==============================] - 67s 6ms/step - loss: 5.8433 - accuracy: 0.1206 - val_loss: 6.2800 - val_accuracy: 0.1158\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x7ee39652da80>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z67u48sa5760"
      },
      "outputs": [],
      "source": [
        "# Sauvegarde du CBOW\n",
        "\n",
        "model.save_weights('/content/drive/My Drive/weights.h5')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Création de clusters bi-grammes\n",
        "\n",
        "import numpy as np\n",
        "from nltk import ngrams\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "negative_comments = df[df['sentiment'] == 'negatif']['cleaned_lemma']\n",
        "\n",
        "# Extraction des embeddings\n",
        "embeddings = model.layers[0].get_weights()[0]\n",
        "\n",
        "n = 2  # bigrammes\n",
        "n_gram_list = []\n",
        "for comment in negative_comments:\n",
        "    words = text_to_word_sequence(comment) # tokenization\n",
        "    n_grams = list(ngrams(words, n))\n",
        "    n_gram_list.extend(n_grams)\n",
        "\n",
        "# Représentation vectorielle pour chaque bigramme\n",
        "n_gram_vectors = []\n",
        "for n_gram in n_gram_list:\n",
        "    indices = [tokenizer.word_index.get(word, 0) for word in n_gram]\n",
        "    # Checker si les indices sont valides (< vocab_size)\n",
        "    if all(idx < vocab_size for idx in indices):\n",
        "        word_vectors = embeddings[indices]\n",
        "        n_gram_vector = np.mean(word_vectors, axis=0)\n",
        "        n_gram_vectors.append(n_gram_vector)\n",
        "\n",
        "# Standardisation des vecteurs de bigrammes\n",
        "scaler = StandardScaler()\n",
        "n_gram_vectors_scaled = scaler.fit_transform(n_gram_vectors)\n"
      ],
      "metadata": {
        "id": "mWyCsRn16p4A"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PCA pour réduire la dimensionnalité\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.95)  # Conserver 95% de la variance explicative\n",
        "n_gram_vectors_pca = pca.fit_transform(n_gram_vectors_scaled)\n",
        "\n",
        "# Clustering avec KMeans sur les données réduites\n",
        "k = 5  # Nombre de clusters\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(n_gram_vectors_pca)\n",
        "clusters = kmeans.labels_\n",
        "\n",
        "# Affichage des clusters\n",
        "cluster_n_grams = {i: [] for i in range(k)}\n",
        "for i, label in enumerate(clusters):\n",
        "    n_gram = n_gram_list[i]\n",
        "    cluster_n_grams[label].append(\" \".join(n_gram))\n",
        "\n",
        "# Afficher les n-grammes pour chaque cluster\n",
        "for cluster, n_grams in cluster_n_grams.items():\n",
        "    print(f\"Cluster {cluster}:\")\n",
        "    for n_gram in n_grams[:10]:  # Afficher les 10 premiers n-grammes pour concision\n",
        "        print(n_gram)\n",
        "    print(\"\\n---\\n\")"
      ],
      "metadata": {
        "id": "X-SlCdvP6qsG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aa070188-c721-4161-cbd8-2c62fd7d270f"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0:\n",
            "commander mardi\n",
            "mardi fevrier\n",
            "jeudi mars\n",
            "non recu\n",
            "recu mars\n",
            "mars prix\n",
            "incapable chronopost\n",
            "pareilservic client\n",
            "client veule\n",
            "patient journee\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 1:\n",
            "fevrier livraison\n",
            "livraison jeudi\n",
            "chronopost pareilservic\n",
            "retouramazon fuir\n",
            "fuir dernier\n",
            "vouloir acheter\n",
            "acheter americain\n",
            "dire signer\n",
            "signer soin\n",
            "gros objet\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 2:\n",
            "methode voleur\n",
            "voleur voyou\n",
            "telephone incompetent\n",
            "incompetent version\n",
            "recevoir service\n",
            "service client\n",
            "faible rapport\n",
            "conforme legislation\n",
            "impossible service\n",
            "service client\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 3:\n",
            "mars produit\n",
            "produit non\n",
            "renvoyer article\n",
            "article semaine\n",
            "version jour\n",
            "jour retouramazon\n",
            "blanc jusque\n",
            "jusque raccrocher\n",
            "amazone voler\n",
            "voler dernier\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 4:\n",
            "colis commander\n",
            "prix colis\n",
            "colis incapable\n",
            "veule patient\n",
            "balle amazonamazon\n",
            "remboursement methode\n",
            "raccrocher appel\n",
            "saturer droite\n",
            "droite faible\n",
            "disque dur\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Définition des noms de clusters basés sur les thèmes identifiés\n",
        "cluster_names = {\n",
        "    0: \"Problèmes de délai ou de réception\",\n",
        "    1: \"Déception du service de livraison\",\n",
        "    2: \"Critiques sévères du service client\",\n",
        "    3: \"Retours et remboursements\",\n",
        "    4: \"Produits défectueux et insatisfaction générale\"\n",
        "}"
      ],
      "metadata": {
        "id": "MSjF-8cfsmoB"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Score de silhouette\n",
        "\n",
        "# Calcul long d'où échantillonnage des données\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "# Calculer le score de silhouette avec un échantillon limité\n",
        "score = silhouette_score(n_gram_vectors_pca, kmeans.labels_, sample_size=50000)\n",
        "print(f\"Score de silhouette (limité): {score}\")\n"
      ],
      "metadata": {
        "id": "I87-X-bjAr9S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db3e31e6-43ec-4492-dbda-aa016fac1034"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Score de silhouette (limité): 0.02242829462556348\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test des clusters formés\n",
        "\n",
        "ref_df = pd.read_csv('/content/drive/My Drive/Datasets NLP/ecommerce_plaintes.csv')\n",
        "\n",
        "# Accès aux commentaires\n",
        "reference_sentences = ref_df['commentaire'].tolist()\n",
        "\n",
        "# Tokenisation et conversion en embeddings pour les phrases de référence\n",
        "ref_n_gram_vectors = []\n",
        "\n",
        "for sentence in reference_sentences:\n",
        "    words = text_to_word_sequence(sentence)\n",
        "    n_grams = list(ngrams(words, n))  # n étant bigramme\n",
        "    sentence_vectors = []\n",
        "    for n_gram in n_grams:\n",
        "        indices = [tokenizer.word_index.get(word, 0) for word in n_gram]\n",
        "        if all(idx < vocab_size for idx in indices):\n",
        "            word_vectors = embeddings[indices]\n",
        "            n_gram_vector = np.mean(word_vectors, axis=0)\n",
        "            sentence_vectors.append(n_gram_vector)\n",
        "    # Moyenne des embeddings des n-grammes pour obtenir un vecteur par phrase\n",
        "    if sentence_vectors:\n",
        "        ref_n_gram_vectors.append(np.mean(sentence_vectors, axis=0))\n",
        "\n",
        "ref_n_gram_vectors = np.array(ref_n_gram_vectors)\n"
      ],
      "metadata": {
        "id": "DUsJ1xL4AvWg"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Même objet PCA que pour les données d'entraînement\n",
        "\n",
        "ref_n_gram_vectors_pca = pca.transform(ref_n_gram_vectors)"
      ],
      "metadata": {
        "id": "UyEmCNqxAxsD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calcul de similarité\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Calculer la similarité de chaque phrase de référence avec les centroïdes des clusters\n",
        "similarities = cosine_similarity(ref_n_gram_vectors_pca, kmeans.cluster_centers_)\n",
        "\n",
        "# Pour chaque phrase, identification du cluster le plus proche\n",
        "closest_clusters = np.argmax(similarities, axis=1)\n",
        "\n",
        "for i, cluster_num in enumerate(closest_clusters):\n",
        "    print(f\"\\nPhrase '{reference_sentences[i]}' est la plus proche de la problématique '{cluster_names[cluster_num]}'\\n\")\n",
        "\n"
      ],
      "metadata": {
        "id": "7rL2hBFvAz-c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "58f7b090-93fd-417d-e689-0ace1850244f"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Phrase 'J'ai attendu plus d'un mois pour recevoir ma commande, c'est inacceptable.' est la plus proche de la problématique 'Critiques sévères du service client'\n",
            "\n",
            "\n",
            "Phrase 'Le suivi de mon colis indique qu'il a été livré, mais je n'ai rien reçu.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Ma commande est restée en statut 'expédiée' pendant trois semaines.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'La livraison était prévue pour hier, mais je n'ai toujours pas mon colis.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le délai de livraison promis n'a pas été respecté, je suis très déçu.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Mon article est arrivé cassé, comment cela a-t-il pu arriver?' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le produit reçu est endommagé, je demande un remboursement immédiat.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'J'ai reçu ma commande, mais l'article est rayé et semble utilisé.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le colis était visiblement endommagé, et l'article à l'intérieur également.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'J'ai attendu longtemps pour ça? Un produit abîmé, super…' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Ça fait des semaines que j'attends mon remboursement, où est-il?' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le service client m'a promis un remboursement, mais je n'ai rien reçu.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'J'ai renvoyé l'article mais toujours pas de nouvelles concernant mon remboursement' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Ils refusent de me rembourser alors que l'article est défectueux!' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Mon remboursement a été injustement refusé, je suis furieux.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'J'ai reçu un article que je n'avais pas commandé, comment est-ce possible?' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Ma commande est incomplète, il manque plusieurs articles.' est la plus proche de la problématique 'Critiques sévères du service client'\n",
            "\n",
            "\n",
            "Phrase 'Ils m'ont envoyé la mauvaise taille, je dois tout renvoyer maintenant.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Au lieu de la couleur bleue commandée, j'ai reçu un article rouge.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Il y a eu une erreur dans ma commande, et le service client ne répond pas.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Impossible de joindre le service client, leur ligne est toujours occupée.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le service client n'a pas résolu mon problème, je suis déçu.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Après plusieurs emails, le service client n'a toujours pas répondu.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le service client m'a raccroché au nez, incroyable!' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le service client est incompétent, ils n'ont pas su répondre à ma question.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Ma carte a été débitée deux fois pour la même commande.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'J'ai été facturé pour des articles que je n'ai jamais reçus.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le site a refusé mon paiement sans raison valable.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Des frais supplémentaires ont été ajoutés sans mon consentement.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le remboursement sur ma carte de crédit prend une éternité.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le site est tellement lent, c'est frustrant.' est la plus proche de la problématique 'Retours et remboursements'\n",
            "\n",
            "\n",
            "Phrase 'J'ai eu du mal à trouver les produits que je voulais, leur moteur de recherche est mauvais.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le site a planté juste au moment où je finalisais ma commande.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'L'interface utilisateur est peu intuitive, je me perds tout le temps.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le site n'est pas du tout adapté aux mobiles, impossible de commander depuis mon téléphone.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Après avoir passé ma commande, on m'informe que l'article est en rupture de stock.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Ils affichent des produits disponibles qui ne le sont pas en réalité.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'J'ai commandé un article en stock, et maintenant ils disent qu'il est en rupture?' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Toujours en attente de la remise en stock promise depuis des mois.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le manque de stock pour les articles populaires est vraiment frustrant.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le produit reçu ne correspond pas du tout à la description sur le site.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Les images du produit étaient trompeuses, très déçu du résultat.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'L'article semblait de qualité sur le site, mais c'est tout le contraire.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'La publicité disait 'livraison gratuite', puis au paiement, surprise, des frais!' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Les caractéristiques du produit annoncées étaient exagérées, ce n'est pas ce que j'ai reçu.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'J'ai reçu des spams après m'être inscrit sur leur site, coïncidence?' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Mes données personnelles ont été partagées sans mon consentement.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Je me suis fait pirater mon compte après avoir acheté sur ce site.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Ils demandent trop d'informations personnelles pour passer commande.' est la plus proche de la problématique 'Critiques sévères du service client'\n",
            "\n",
            "\n",
            "Phrase 'Après un achat, mon adresse email a été inondée d'offres commerciales non sollicitées' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Préparation des vecteurs de commentaires pour l'affichage des clusters par entreprise\n",
        "\n",
        "for comment in df['cleaned_lemma'][df['sentiment'] == 'negatif']:\n",
        "    words = text_to_word_sequence(comment)  # Tokenisation\n",
        "    word_indices = [tokenizer.word_index[word] for word in words if word in tokenizer.word_index]\n",
        "    # Filtrer les indices qui sont dans la plage valide\n",
        "    valid_indices = [idx for idx in word_indices if idx < len(embeddings)]\n",
        "    if valid_indices:\n",
        "        comment_vector = np.mean([embeddings[idx] for idx in valid_indices], axis=0)\n",
        "        comment_vectors.append(comment_vector)\n",
        "\n"
      ],
      "metadata": {
        "id": "7C0oUJzRuuwq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "valid_indices = []\n",
        "\n",
        "# Génération des vecteurs de commentaires\n",
        "comment_vectors = []\n",
        "for i, comment in enumerate(df['cleaned_lemma'][df['sentiment'] == 'negatif']):\n",
        "    words = text_to_word_sequence(comment)  # Tokenisation\n",
        "    word_indices = [tokenizer.word_index.get(word, -1) for word in words]\n",
        "    valid_word_indices = [idx for idx in word_indices if 0 <= idx < len(embeddings)]\n",
        "    if valid_word_indices:\n",
        "        comment_vector = np.mean([embeddings[idx] for idx in valid_word_indices], axis=0)\n",
        "        comment_vectors.append(comment_vector)\n",
        "        valid_indices.append(i)  # Stocker l'indice du commentaire valide\n",
        "\n",
        "comment_vectors = np.array(comment_vectors)\n",
        "\n",
        "# Standardisation des vecteurs de commentaires\n",
        "scaler = StandardScaler()\n",
        "comment_vectors_scaled = scaler.fit_transform(comment_vectors)\n",
        "\n",
        "# Clustering avec KMeans\n",
        "kmeans = KMeans(n_clusters=10, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(comment_vectors_scaled)\n",
        "\n",
        "# Association des clusters aux commentaires\n",
        "df_negatif_valid = df[df['sentiment'] == 'negatif'].iloc[valid_indices].copy()\n",
        "df_negatif_valid['cluster'] = clusters\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nlYSaN5atu0P"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyse des clusters par entreprise\n",
        "cluster_company_distribution = df_negatif_valid.groupby(['cluster', 'company']).size().unstack(fill_value=0)\n",
        "print(cluster_company_distribution)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-vvzeHzt-o_",
        "outputId": "9cad1233-e2a4-4436-ccab-a415b0d0e2c7"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "company  Amazon  CDiscount  Fnac\n",
            "cluster                         \n",
            "0           258        878   230\n",
            "1            23        369    38\n",
            "2           495       1423   394\n",
            "3            13        361     1\n",
            "4          1046        800   223\n",
            "5           216        725    81\n",
            "6          2850       7998  2941\n",
            "7           688       1499   466\n",
            "8           245       1100   255\n",
            "9           317       1528   276\n"
          ]
        }
      ]
    }
  ]
}