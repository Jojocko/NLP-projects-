{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jojocko/NLP-projects-/blob/main/Suite_we_cbow_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmZ-Nzej6EP_",
        "outputId": "3452d341-4d1c-4fc0-86f0-97dbba3d632f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/Datasets NLP/dataset_supplychain.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFCpZITa6WAJ"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['client', 'langage', 'reponse'], axis=1)\n",
        "df.rename(columns={'Sentiment': 'sentiment', 'Commentaire': 'commentaire'}, inplace=True)\n",
        "df['sentiment'] = df['sentiment'].replace({'__label__POSITIVE': 'positif', '__label__NEGATIVE': 'negatif', '__label__NEUTRAL': 'neutre'})\n",
        "df['date'] = df['date'].fillna(method=\"ffill\")\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'] = df['date'].dt.year # pour visualiser par année et non par jour/mois/année"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd4K5P597rV3",
        "outputId": "311b835e-d90b-4bf0-a130-b0b2f49ce25b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('french'))\n",
        "\n",
        "jugement = {'très', 'extrêmement', 'particulièrement', 'exceptionnellement','tout à fait', 'absolument', 'complètement', 'entièrement', 'parfaitement', 'profondément', 'hautement', 'tout', 'plutôt', 'assez', 'bien', 'bon','vraiment', 'totalement', 'énormément', 'peu', 'moins'}\n",
        "satisfaction = {'satisfait', 'content', 'heureux', 'ravi', 'enchanté', 'comblé', 'agréable', 'plaisant', 'positif', 'excellent', 'remarquable', 'exceptionnel', 'superbe', 'admirable', 'réjoui', 'gratifiant', 'récompensant', 'conquis', 'impressionné', 'élogieux'}\n",
        "insatisfaction = {'insatisfait', 'mécontent', 'déçu', 'frustré', 'contrarié', 'désappointé', 'inacceptable', 'problématique', 'inadmissible', 'déplorable', 'lamentable', 'irrité', 'en colère', 'révolté', 'amère', 'négatif', 'critique', 'malheureux', 'peu convaincu', 'regrettable'}\n",
        "company = {'Fnac', 'fnac', 'Amazon', 'amazon', 'CDiscount', 'cdiscount'}\n",
        "\n",
        "stop_words.update(jugement, satisfaction, insatisfaction, company)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9TjZYw5e6bjW"
      },
      "outputs": [],
      "source": [
        "# Traitement des données pour clustering des commentaires négatifs\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip()) # lower capitalized letters + retire les espaces en début et fin de string (phrases)\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w) # ajout d'espaces autour des ponctuations pour les séparer des mots. eg: \"he is a boy.\" => \"he is a boy .\n",
        "    w = re.sub(r'[\" \"]+', \" \", w) # remplacement des séquences d'espaces multiples par un seul espace\n",
        "    w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w) # suppression de tout caractère qui n'est pas une lettre ou une ponctuation courante\n",
        "    w = re.sub(r'\\b\\w{0,2}\\b', '', w) # suppression des mots de moins de trois lettres.\n",
        "\n",
        "    # remove stopword\n",
        "    mots = word_tokenize(w.strip()) # tokénization en mots individuels\n",
        "    mots = [mot for mot in mots if mot not in stop_words] # filtrage des mots vides\n",
        "    return ' '.join(mots).strip() # reconstruction de la phrase sans les mots vides\n",
        "\n",
        "df.cleaned_lemma = df.cleaned_lemma.apply(lambda x :preprocess_sentence(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lWAY5ck72pB",
        "outputId": "2567d8ef-aab2-4dbf-824b-4100276d294c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (764323, 4)\n",
            "Shape of Y: (764323, 1)\n"
          ]
        }
      ],
      "source": [
        "# Création de l'ensemble de données pour archtecture CBOW\n",
        "\n",
        "# Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000) # doc limité à 10000 mots\n",
        "tokenizer.fit_on_texts(df.cleaned_lemma) # application du dic sur les commentaires\n",
        "\n",
        "word2idx = tokenizer.word_index # stocke le dictionnaire de correspondance entre mots et index\n",
        "idx2word = tokenizer.index_word # stocke le dictionnaire de correspondance entre index et mots\n",
        "vocab_size = tokenizer.num_words # stocke la taille du dictionnaire\n",
        "\n",
        "# Ensemble de données\n",
        "import numpy as np\n",
        "\n",
        "def sentenceToData(tokens, WINDOW_SIZE):\n",
        "    context_size = WINDOW_SIZE // 2\n",
        "    window = np.concatenate((np.arange(-context_size, 0), np.arange(1, context_size + 1)))\n",
        "    X, Y = [], []\n",
        "    for word_index, word in enumerate(tokens):\n",
        "        if (word_index - context_size >= 0) and (word_index + context_size < len(tokens)):\n",
        "            context = [tokens[word_index + offset] for offset in window]\n",
        "            target = word\n",
        "            X.append(context)\n",
        "            Y.append(target)\n",
        "    return X, Y\n",
        "\n",
        "WINDOW_SIZE = 5  # Fenêtre de contexte autour du mot cible\n",
        "\n",
        "X, Y = [], []\n",
        "for review in df['cleaned_lemma'][df['sentiment'] == 'negatif']:\n",
        "    sentences = review.split(\".\")  # transforme les commentaires en phrases\n",
        "    for sentence in sentences:\n",
        "        word_list = tokenizer.texts_to_sequences([sentence.strip()])[0]  # convertit les phrases en séquence\n",
        "        if len(word_list) >= WINDOW_SIZE:\n",
        "            X_temp, Y_temp = sentenceToData(word_list, WINDOW_SIZE)\n",
        "            X.extend(X_temp)\n",
        "            Y.extend(Y_temp)\n",
        "\n",
        "# Convert X and Y to numpy arrays\n",
        "X = np.array(X).astype(int)\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X = pad_sequences(X, padding='post')\n",
        "Y = np.array(Y).reshape(-1, 1)\n",
        "\n",
        "print('Shape of X:', X.shape)\n",
        "print('Shape of Y:', Y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T77NUTOc9WF2",
        "outputId": "80698626-d609-40ac-d83e-de0fd6f1d118"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 512)         5120512   \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 128)               246528    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               33024     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10001)             2570257   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7970321 (30.40 MB)\n",
            "Trainable params: 2849809 (10.87 MB)\n",
            "Non-trainable params: 5120512 (19.53 MB)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Architecture CBOW\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Dropout, GRU\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "vocab_size = min(len(word2idx) + 1, tokenizer.num_words + 1)\n",
        "embedding_dim = 512\n",
        "\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "embedding_layer = Embedding(input_dim=vocab_size, output_dim=embedding_dim)\n",
        "embedding_layer.trainable = False\n",
        "\n",
        "\n",
        "model = Sequential()\n",
        "model.add(embedding_layer)\n",
        "model.add(GRU(units=128, return_sequences=False))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z67u48sa5760"
      },
      "outputs": [],
      "source": [
        "# Chargement du du CBOW\n",
        "\n",
        "model.load_weights('/content/drive/My Drive/weights.h5')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWyCsRn16p4A"
      },
      "outputs": [],
      "source": [
        "# Création de clusters bi-grammes\n",
        "\n",
        "import numpy as np\n",
        "from nltk import ngrams\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "negative_comments = df[df['sentiment'] == 'negatif']['cleaned_lemma']\n",
        "\n",
        "# Extraction des embeddings\n",
        "embeddings = model.layers[0].get_weights()[0]\n",
        "\n",
        "n = 2  # bigrammes\n",
        "n_gram_list = []\n",
        "for comment in negative_comments:\n",
        "    words = text_to_word_sequence(comment) # tokenization\n",
        "    n_grams = list(ngrams(words, n))\n",
        "    n_gram_list.extend(n_grams)\n",
        "\n",
        "# Représentation vectorielle pour chaque bigramme\n",
        "n_gram_vectors = []\n",
        "for n_gram in n_gram_list:\n",
        "    indices = [tokenizer.word_index.get(word, 0) for word in n_gram]\n",
        "    # Checker si les indices sont valides (< vocab_size)\n",
        "    if all(idx < vocab_size for idx in indices):\n",
        "        word_vectors = embeddings[indices]\n",
        "        n_gram_vector = np.mean(word_vectors, axis=0)\n",
        "        n_gram_vectors.append(n_gram_vector)\n",
        "\n",
        "# Standardisation des vecteurs de bigrammes\n",
        "scaler = StandardScaler()\n",
        "n_gram_vectors_scaled = scaler.fit_transform(n_gram_vectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-SlCdvP6qsG",
        "outputId": "b8da5e64-6321-4f70-cd73-a5a33e507955"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0:\n",
            "mars produit\n",
            "produit non\n",
            "prix colis\n",
            "colis incapable\n",
            "chronopost pareilservic\n",
            "veule patient\n",
            "balle amazonamazon\n",
            "retouramazon fuir\n",
            "fuir dernier\n",
            "raccrocher appel\n",
            "dire signer\n",
            "signer soin\n",
            "gros objet\n",
            "vendeur conforme\n",
            "legislation francaise\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 1:\n",
            "attendre colis\n",
            "politique pire\n",
            "pire contrairement\n",
            "rendre jour\n",
            "saturer droite\n",
            "droite faible\n",
            "ecrire renvoyer\n",
            "renvoyer disque\n",
            "disque dur\n",
            "dur reparable\n",
            "repondre relance\n",
            "relance reponse\n",
            "pouvoir rien\n",
            "client discussion\n",
            "discussion ligne\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 2:\n",
            "version jour\n",
            "jour retouramazon\n",
            "voler dernier\n",
            "dernier commande\n",
            "marche tete\n",
            "mauvais experience\n",
            "retrouver rue\n",
            "rue retrouver\n",
            "version telechargement\n",
            "telechargement livre\n",
            "frauduleux service\n",
            "service client\n",
            "commander tube\n",
            "transporteur chronopost\n",
            "chronopost resoudre\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 3:\n",
            "methode voleur\n",
            "voleur voyou\n",
            "telephone incompetent\n",
            "incompetent version\n",
            "recevoir service\n",
            "service client\n",
            "faible rapport\n",
            "conforme legislation\n",
            "impossible service\n",
            "service client\n",
            "magnifique plomber\n",
            "plomber incroyable\n",
            "aider marche\n",
            "tube rubson\n",
            "rubson livrer\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 4:\n",
            "colis commander\n",
            "commander mardi\n",
            "fevrier livraison\n",
            "livraison jeudi\n",
            "vouloir acheter\n",
            "acheter americain\n",
            "objet livreur\n",
            "livreur venir\n",
            "reponse dernier\n",
            "dernier pouvoir\n",
            "brouillonner laborieux\n",
            "laborieux interlocuteur\n",
            "interlocuteur maitriser\n",
            "maitriser francais\n",
            "service client\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 5:\n",
            "rembourser moque\n",
            "moque client\n",
            "suivi colis\n",
            "colis impossible\n",
            "visible agir\n",
            "agir cheminee\n",
            "jour confiance\n",
            "confiance amazone\n",
            "inconnu entendre\n",
            "entendre piratage\n",
            "non livrer\n",
            "livrer fois\n",
            "augmenter telephoner\n",
            "telephoner non\n",
            "faire rembourser\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 6:\n",
            "mardi fevrier\n",
            "jeudi mars\n",
            "non recu\n",
            "recu mars\n",
            "mars prix\n",
            "incapable chronopost\n",
            "pareilservic client\n",
            "client veule\n",
            "patient journee\n",
            "journee plusle\n",
            "plusle colis\n",
            "colis introuvable\n",
            "introuvable chronopost\n",
            "chronopost renvoyer\n",
            "renvoyer balle\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 7:\n",
            "renvoyer article\n",
            "article semaine\n",
            "faire annuler\n",
            "annuler commande\n",
            "rire jaune\n",
            "rappeler jour\n",
            "renvoyer encre\n",
            "encre imprimante\n",
            "telephone portable\n",
            "cloudear expedier\n",
            "expedier notice\n",
            "and difficult\n",
            "difficult assemblei\n",
            "parcours combattant\n",
            "combattant aucun\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# PCA pour réduire la dimensionnalité\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.95)  # Conserver 95% de la variance explicative\n",
        "n_gram_vectors_pca = pca.fit_transform(n_gram_vectors_scaled)\n",
        "\n",
        "# Clustering avec KMeans sur les données réduites\n",
        "k = 8  # Nombre de clusters\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(n_gram_vectors_pca)\n",
        "clusters = kmeans.labels_\n",
        "\n",
        "# Affichage des clusters\n",
        "cluster_n_grams = {i: [] for i in range(k)}\n",
        "for i, label in enumerate(clusters):\n",
        "    n_gram = n_gram_list[i]\n",
        "    cluster_n_grams[label].append(\" \".join(n_gram))\n",
        "\n",
        "# Afficher les n-grammes pour chaque cluster\n",
        "for cluster, n_grams in cluster_n_grams.items():\n",
        "    print(f\"Cluster {cluster}:\")\n",
        "    for n_gram in n_grams[:15]:\n",
        "        print(n_gram)\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MSjF-8cfsmoB"
      },
      "outputs": [],
      "source": [
        "# Définition des noms de clusters basés sur les thèmes identifiés\n",
        "cluster_names = {\n",
        "    0: \"Problèmes de conformité\",\n",
        "    1: \"Qualité défectueuse et retours\",\n",
        "    2: \"Déception du service de livraison\",\n",
        "    3: \"Critiques sévères du service client\",\n",
        "    4: \"Délais et difficultés de communication\",\n",
        "    5: \"Déception et remboursements\",\n",
        "    6: \"Problèmes de suivi de commande\",\n",
        "    7: \"Difficultés de l'expérience d'achat\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I87-X-bjAr9S",
        "outputId": "db3e31e6-43ec-4492-dbda-aa016fac1034"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score de silhouette (limité): 0.02242829462556348\n"
          ]
        }
      ],
      "source": [
        "# Score de silhouette\n",
        "\n",
        "# Calcul long d'où échantillonnage des données\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "score = silhouette_score(n_gram_vectors_pca, kmeans.labels_, sample_size=50000)\n",
        "print(f\"Score de silhouette: {score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DUsJ1xL4AvWg"
      },
      "outputs": [],
      "source": [
        "# Test des clusters formés avec phrases de référence\n",
        "\n",
        "ref_df = pd.read_csv('/content/drive/My Drive/Datasets NLP/ecommerce_plaintes.csv')\n",
        "\n",
        "reference_sentences = ref_df['commentaire'].tolist()\n",
        "\n",
        "# Tokenisation et conversion en embeddings\n",
        "ref_n_gram_vectors = []\n",
        "\n",
        "for sentence in reference_sentences:\n",
        "    words = text_to_word_sequence(sentence)\n",
        "    n_grams = list(ngrams(words, n))  # n étant bigramme\n",
        "    sentence_vectors = []\n",
        "    for n_gram in n_grams:\n",
        "        indices = [tokenizer.word_index.get(word, 0) for word in n_gram]\n",
        "        if all(idx < vocab_size for idx in indices):\n",
        "            word_vectors = embeddings[indices]\n",
        "            n_gram_vector = np.mean(word_vectors, axis=0)\n",
        "            sentence_vectors.append(n_gram_vector)\n",
        "    # Moyenne des embeddings des n-grammes pour obtenir un vecteur par phrase\n",
        "    if sentence_vectors:\n",
        "        ref_n_gram_vectors.append(np.mean(sentence_vectors, axis=0))\n",
        "\n",
        "ref_n_gram_vectors = np.array(ref_n_gram_vectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UyEmCNqxAxsD"
      },
      "outputs": [],
      "source": [
        "# Même objet PCA que pour les données d'entraînement\n",
        "\n",
        "ref_n_gram_vectors_pca = pca.transform(ref_n_gram_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rL2hBFvAz-c",
        "outputId": "a6a52e8b-b24a-4b69-f560-954670c55af6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Phrase 'J'ai attendu plus d'un mois pour recevoir ma commande, c'est inacceptable.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le suivi de mon colis indique qu'il a été livré, mais je n'ai rien reçu.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Ma commande est restée en statut 'expédiée' pendant trois semaines.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'La livraison était prévue pour hier, mais je n'ai toujours pas mon colis.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le délai de livraison promis n'a pas été respecté, je suis très déçu.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Mon article est arrivé cassé, comment cela a-t-il pu arriver?' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le produit reçu est endommagé, je demande un remboursement immédiat.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'J'ai reçu ma commande, mais l'article est rayé et semble utilisé.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le colis était visiblement endommagé, et l'article à l'intérieur également.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'J'ai attendu longtemps pour ça? Un produit abîmé, super…' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Ça fait des semaines que j'attends mon remboursement, où est-il?' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le service client m'a promis un remboursement, mais je n'ai rien reçu.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'J'ai renvoyé l'article mais toujours pas de nouvelles concernant mon remboursement' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Ils refusent de me rembourser alors que l'article est défectueux!' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Mon remboursement a été injustement refusé, je suis furieux.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'J'ai reçu un article que je n'avais pas commandé, comment est-ce possible?' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Ma commande est incomplète, il manque plusieurs articles.' est la plus proche de la problématique 'Critiques sévères du service client'\n",
            "\n",
            "\n",
            "Phrase 'Ils m'ont envoyé la mauvaise taille, je dois tout renvoyer maintenant.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Au lieu de la couleur bleue commandée, j'ai reçu un article rouge.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Il y a eu une erreur dans ma commande, et le service client ne répond pas.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Impossible de joindre le service client, leur ligne est toujours occupée.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le service client n'a pas résolu mon problème, je suis déçu.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Après plusieurs emails, le service client n'a toujours pas répondu.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le service client m'a raccroché au nez, incroyable!' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le service client est incompétent, ils n'ont pas su répondre à ma question.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Ma carte a été débitée deux fois pour la même commande.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'J'ai été facturé pour des articles que je n'ai jamais reçus.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le site a refusé mon paiement sans raison valable.' est la plus proche de la problématique 'Difficultés de l'expérience d'achat'\n",
            "\n",
            "\n",
            "Phrase 'Des frais supplémentaires ont été ajoutés sans mon consentement.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le remboursement sur ma carte de crédit prend une éternité.' est la plus proche de la problématique 'Qualité défectueuse et retours'\n",
            "\n",
            "\n",
            "Phrase 'Le site est tellement lent, c'est frustrant.' est la plus proche de la problématique 'Difficultés de l'expérience d'achat'\n",
            "\n",
            "\n",
            "Phrase 'J'ai eu du mal à trouver les produits que je voulais, leur moteur de recherche est mauvais.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le site a planté juste au moment où je finalisais ma commande.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'L'interface utilisateur est peu intuitive, je me perds tout le temps.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le site n'est pas du tout adapté aux mobiles, impossible de commander depuis mon téléphone.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Après avoir passé ma commande, on m'informe que l'article est en rupture de stock.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Ils affichent des produits disponibles qui ne le sont pas en réalité.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'J'ai commandé un article en stock, et maintenant ils disent qu'il est en rupture?' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Toujours en attente de la remise en stock promise depuis des mois.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le manque de stock pour les articles populaires est vraiment frustrant.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le produit reçu ne correspond pas du tout à la description sur le site.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Les images du produit étaient trompeuses, très déçu du résultat.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'L'article semblait de qualité sur le site, mais c'est tout le contraire.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'La publicité disait 'livraison gratuite', puis au paiement, surprise, des frais!' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Les caractéristiques du produit annoncées étaient exagérées, ce n'est pas ce que j'ai reçu.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'J'ai reçu des spams après m'être inscrit sur leur site, coïncidence?' est la plus proche de la problématique 'Difficultés de l'expérience d'achat'\n",
            "\n",
            "\n",
            "Phrase 'Mes données personnelles ont été partagées sans mon consentement.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Je me suis fait pirater mon compte après avoir acheté sur ce site.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Ils demandent trop d'informations personnelles pour passer commande.' est la plus proche de la problématique 'Critiques sévères du service client'\n",
            "\n",
            "\n",
            "Phrase 'Après un achat, mon adresse email a été inondée d'offres commerciales non sollicitées' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Calcul de similarité\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "# Calculer la similarité de chaque phrase de référence avec les centroïdes des clusters\n",
        "similarities = cosine_similarity(ref_n_gram_vectors_pca, kmeans.cluster_centers_)\n",
        "\n",
        "# Identification du cluster le plus proche\n",
        "closest_clusters = np.argmax(similarities, axis=1)\n",
        "\n",
        "for i, cluster_num in enumerate(closest_clusters):\n",
        "    print(f\"\\nPhrase '{reference_sentences[i]}' est la plus proche de la problématique '{cluster_names[cluster_num]}'\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7C0oUJzRuuwq"
      },
      "outputs": [],
      "source": [
        "# Préparation des vecteurs de commentaires pour l'affichage des clusters par entreprise\n",
        "\n",
        "valid_indices = []\n",
        "comment_vectors = []\n",
        "\n",
        "# Génération des vecteurs de commentaires\n",
        "for i, comment in enumerate(df['cleaned_lemma'][df['sentiment'] == 'negatif']):\n",
        "    words = text_to_word_sequence(comment)  # Tokenisation\n",
        "    word_indices = [tokenizer.word_index.get(word, -1) for word in words]\n",
        "    valid_word_indices = [idx for idx in word_indices if 0 <= idx < len(embeddings)]\n",
        "    if valid_word_indices:\n",
        "        comment_vector = np.mean([embeddings[idx] for idx in valid_word_indices], axis=0)\n",
        "        comment_vectors.append(comment_vector)\n",
        "        valid_indices.append(i)  # Correctly storing index of the comment that contributes to comment_vectors\n",
        "\n",
        "comment_vectors = np.array(comment_vectors)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "comment_vectors_scaled = scaler.fit_transform(comment_vectors)\n",
        "\n",
        "kmeans = KMeans(n_clusters=8, random_state=42, n_init=10)\n",
        "clusters = kmeans.fit_predict(comment_vectors_scaled)\n",
        "\n",
        "df_negatif_valid = df[df['sentiment'] == 'negatif'].iloc[valid_indices].copy()\n",
        "df_negatif_valid['cluster'] = clusters\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changement des noms des clusters dans la colonne df\n",
        "\n",
        "df_negatif_valid['cluster'] = df_negatif_valid['cluster'].replace(cluster_names)\n",
        "\n"
      ],
      "metadata": {
        "id": "1lpsHj4bepjC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-vvzeHzt-o_",
        "outputId": "e862f302-8055-4c6f-8d70-0c8489285f7b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "company                                 Amazon  CDiscount  Fnac\n",
            "cluster                                                        \n",
            "Critiques sévères du service client       1079        846   237\n",
            "Difficultés de l'expérience d'achat        260        890   233\n",
            "Déception du service de livraison          742       1600   500\n",
            "Déception et remboursements               3017       9177  3146\n",
            "Délais et difficultés de communication     225        770    85\n",
            "Problèmes de conformité                    267       1211   279\n",
            "Problèmes de suivi de commande              27        674    12\n",
            "Qualité défectueuse et retours             534       1513   413\n"
          ]
        }
      ],
      "source": [
        "# Analyse des clusters par entreprise\n",
        "cluster_company_distribution = df_negatif_valid.groupby(['cluster', 'company']).size().unstack(fill_value=0)\n",
        "print(cluster_company_distribution)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyPo10he7o+Px7MfYWL2YbmF",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}