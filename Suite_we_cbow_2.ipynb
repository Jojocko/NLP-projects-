{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Jojocko/NLP-projects-/blob/main/Suite_we_cbow_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmZ-Nzej6EP_",
        "outputId": "9ddcacc2-9849-4e43-ebd3-7a44524fbe3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv('/content/drive/My Drive/Datasets NLP/dataset_supplychain.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "PFCpZITa6WAJ"
      },
      "outputs": [],
      "source": [
        "df = df.drop(['client', 'langage', 'reponse'], axis=1)\n",
        "df.rename(columns={'Sentiment': 'sentiment', 'Commentaire': 'commentaire'}, inplace=True)\n",
        "df['sentiment'] = df['sentiment'].replace({'__label__POSITIVE': 'positif', '__label__NEGATIVE': 'negatif', '__label__NEUTRAL': 'neutre'})\n",
        "df['date'] = df['date'].fillna(method=\"ffill\")\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "df['year'] = df['date'].dt.year # pour une visualisation de l'évolution chronologique plus claire"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qd4K5P597rV3",
        "outputId": "d89cf988-b00b-4639-c237-530cd55abdc0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "\n",
        "stop_words = set(stopwords.words('french'))\n",
        "\n",
        "jugement = {'très', 'extrêmement', 'particulièrement', 'exceptionnellement','tout à fait', 'absolument', 'complètement', 'entièrement', 'parfaitement', 'profondément', 'hautement', 'tout', 'plutôt', 'assez', 'bien', 'bon','vraiment', 'totalement', 'énormément', 'peu', 'moins'}\n",
        "satisfaction = {'satisfait', 'content', 'heureux', 'ravi', 'enchanté', 'comblé', 'agréable', 'plaisant', 'positif', 'excellent', 'remarquable', 'exceptionnel', 'superbe', 'admirable', 'réjoui', 'gratifiant', 'récompensant', 'conquis', 'impressionné', 'élogieux'}\n",
        "insatisfaction = {'insatisfait', 'mécontent', 'déçu', 'frustré', 'contrarié', 'désappointé', 'inacceptable', 'problématique', 'inadmissible', 'déplorable', 'lamentable', 'irrité', 'en colère', 'révolté', 'amère', 'négatif', 'critique', 'malheureux', 'peu convaincu', 'regrettable'}\n",
        "company = {'Fnac', 'fnac', 'Amazon', 'amazon', 'CDiscount', 'cdiscount'}\n",
        "\n",
        "stop_words.update(jugement, satisfaction, insatisfaction, company)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "9TjZYw5e6bjW"
      },
      "outputs": [],
      "source": [
        "# Traitement des données pour clustering des commentaires négatifs\n",
        "\n",
        "import re\n",
        "import unicodedata\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "def unicode_to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w):\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    w = re.sub(r\"[^a-zA-Z?.!]+\", \" \", w)\n",
        "    w = re.sub(r'\\b\\w{0,2}\\b', '', w)\n",
        "\n",
        "    mots = word_tokenize(w.strip())\n",
        "    mots = [mot for mot in mots if mot not in stop_words]\n",
        "    return ' '.join(mots).strip()\n",
        "\n",
        "df.cleaned_lemma = df.cleaned_lemma.apply(lambda x :preprocess_sentence(x))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1lWAY5ck72pB",
        "outputId": "446da96a-85fb-49be-aa3d-ec9ca95b43ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of X: (764323, 4)\n",
            "Shape of Y: (764323, 1)\n"
          ]
        }
      ],
      "source": [
        "# Création de l'ensemble de données pour archtecture CBOW\n",
        "\n",
        "# Tokenizer\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "\n",
        "tokenizer = Tokenizer(num_words=10000)\n",
        "tokenizer.fit_on_texts(df.cleaned_lemma)\n",
        "\n",
        "word2idx = tokenizer.word_index\n",
        "idx2word = tokenizer.index_word\n",
        "vocab_size = tokenizer.num_words\n",
        "\n",
        "# Ensemble de données\n",
        "import numpy as np\n",
        "\n",
        "def contexttoword(tokens, WINDOW_SIZE):\n",
        "    context_size = WINDOW_SIZE // 2\n",
        "    window = np.concatenate((np.arange(-context_size, 0), np.arange(1, context_size + 1)))\n",
        "    X, Y = [], []\n",
        "    for word_index, word in enumerate(tokens):\n",
        "        if (word_index - context_size >= 0) and (word_index + context_size < len(tokens)):\n",
        "            context = [tokens[word_index + offset] for offset in window]\n",
        "            target = word\n",
        "            X.append(context)\n",
        "            Y.append(target)\n",
        "    return X, Y\n",
        "\n",
        "WINDOW_SIZE = 5\n",
        "\n",
        "X, Y = [], []\n",
        "for review in df['cleaned_lemma'][df['sentiment'] == 'negatif']:\n",
        "    sentences = review.split(\".\")\n",
        "    for sentence in sentences:\n",
        "        word_list = tokenizer.texts_to_sequences([sentence.strip()])[0]\n",
        "        if len(word_list) >= WINDOW_SIZE:\n",
        "            X_temp, Y_temp = contexttoword(word_list, WINDOW_SIZE)\n",
        "            X.extend(X_temp)\n",
        "            Y.extend(Y_temp)\n",
        "\n",
        "X = np.array(X).astype(int)\n",
        "Y = np.array(Y).reshape(-1, 1)\n",
        "\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "X = pad_sequences(X, padding='post')\n",
        "\n",
        "print('Shape of X:', X.shape)\n",
        "print('Shape of Y:', Y.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T77NUTOc9WF2",
        "outputId": "3e608125-2123-45cf-e406-fd7ad7eaa3bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, None, 512)         5120512   \n",
            "                                                                 \n",
            " gru (GRU)                   (None, 128)               246528    \n",
            "                                                                 \n",
            " dense (Dense)               (None, 256)               33024     \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 256)               0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 10001)             2570257   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 7970321 (30.40 MB)\n",
            "Trainable params: 7970321 (30.40 MB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "# Entraînement du modèle d'embedding\n",
        "\n",
        "from tensorflow.keras import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Dense, GlobalAveragePooling1D, Dropout, GRU\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "\n",
        "vocab_size = min(len(word2idx) + 1, tokenizer.num_words + 1)\n",
        "embedding_dim = 512\n",
        "optimizer = Adam(learning_rate=0.001)\n",
        "\n",
        "model = Sequential()\n",
        "model.add(Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n",
        "model.add(GRU(units=128, return_sequences=False))\n",
        "model.add(Dense(256, activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(vocab_size, activation='softmax'))\n",
        "\n",
        "model.summary()\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "z67u48sa5760"
      },
      "outputs": [],
      "source": [
        "# Chargement du du CBOW\n",
        "\n",
        "model.load_weights('/content/drive/My Drive/weights.h5')\n",
        "# early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "# model.fit(X, Y, epochs=20, batch_size=512, callbacks=[early_stopping], validation_split=0.1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "mWyCsRn16p4A"
      },
      "outputs": [],
      "source": [
        "# Création de clusters bi-grammes\n",
        "\n",
        "import numpy as np\n",
        "from nltk import ngrams\n",
        "from sklearn.cluster import KMeans\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
        "\n",
        "negative_comments = df[df['sentiment'] == 'negatif']['cleaned_lemma']\n",
        "\n",
        "# Extraction des embeddings\n",
        "embeddings = model.layers[0].get_weights()[0]\n",
        "\n",
        "# Création des bigrammes\n",
        "n = 2\n",
        "n_gram_list = []\n",
        "for comment in negative_comments:\n",
        "    words = text_to_word_sequence(comment) # tokenization\n",
        "    n_grams = list(ngrams(words, n))\n",
        "    n_gram_list.extend(n_grams)\n",
        "\n",
        "# Représentation vectorielle pour chaque bigramme\n",
        "n_gram_vectors = []\n",
        "for n_gram in n_gram_list:\n",
        "    indices = [tokenizer.word_index.get(word, 0) for word in n_gram]\n",
        "    # Checker si les indices sont valides (< vocab_size)\n",
        "    if all(idx < vocab_size for idx in indices):\n",
        "        word_vectors = embeddings[indices]\n",
        "        n_gram_vector = np.mean(word_vectors, axis=0)\n",
        "        n_gram_vectors.append(n_gram_vector)\n",
        "\n",
        "# Standardisation des vecteurs de bigrammes\n",
        "scaler = StandardScaler()\n",
        "n_gram_vectors_scaled = scaler.fit_transform(n_gram_vectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X-SlCdvP6qsG",
        "outputId": "e42e2147-ccc3-4f78-92c5-cc37ca4c3a0a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cluster 0:\n",
            "mars produit\n",
            "produit non\n",
            "prix colis\n",
            "colis incapable\n",
            "chronopost pareilservic\n",
            "veule patient\n",
            "balle amazonamazon\n",
            "retouramazon fuir\n",
            "fuir dernier\n",
            "raccrocher appel\n",
            "dire signer\n",
            "signer soin\n",
            "gros objet\n",
            "vendeur conforme\n",
            "legislation francaise\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 1:\n",
            "attendre colis\n",
            "politique pire\n",
            "pire contrairement\n",
            "rendre jour\n",
            "saturer droite\n",
            "droite faible\n",
            "ecrire renvoyer\n",
            "renvoyer disque\n",
            "disque dur\n",
            "dur reparable\n",
            "repondre relance\n",
            "relance reponse\n",
            "pouvoir rien\n",
            "client discussion\n",
            "discussion ligne\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 2:\n",
            "version jour\n",
            "jour retouramazon\n",
            "voler dernier\n",
            "dernier commande\n",
            "marche tete\n",
            "mauvais experience\n",
            "retrouver rue\n",
            "rue retrouver\n",
            "version telechargement\n",
            "telechargement livre\n",
            "frauduleux service\n",
            "service client\n",
            "commander tube\n",
            "transporteur chronopost\n",
            "chronopost resoudre\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 3:\n",
            "methode voleur\n",
            "voleur voyou\n",
            "telephone incompetent\n",
            "incompetent version\n",
            "recevoir service\n",
            "service client\n",
            "faible rapport\n",
            "conforme legislation\n",
            "impossible service\n",
            "service client\n",
            "magnifique plomber\n",
            "plomber incroyable\n",
            "aider marche\n",
            "tube rubson\n",
            "rubson livrer\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 4:\n",
            "colis commander\n",
            "commander mardi\n",
            "fevrier livraison\n",
            "livraison jeudi\n",
            "vouloir acheter\n",
            "acheter americain\n",
            "objet livreur\n",
            "livreur venir\n",
            "reponse dernier\n",
            "dernier pouvoir\n",
            "brouillonner laborieux\n",
            "laborieux interlocuteur\n",
            "interlocuteur maitriser\n",
            "maitriser francais\n",
            "service client\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 5:\n",
            "rembourser moque\n",
            "moque client\n",
            "suivi colis\n",
            "colis impossible\n",
            "visible agir\n",
            "agir cheminee\n",
            "jour confiance\n",
            "confiance amazone\n",
            "inconnu entendre\n",
            "entendre piratage\n",
            "non livrer\n",
            "livrer fois\n",
            "augmenter telephoner\n",
            "telephoner non\n",
            "faire rembourser\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 6:\n",
            "mardi fevrier\n",
            "jeudi mars\n",
            "non recu\n",
            "recu mars\n",
            "mars prix\n",
            "incapable chronopost\n",
            "pareilservic client\n",
            "client veule\n",
            "patient journee\n",
            "journee plusle\n",
            "plusle colis\n",
            "colis introuvable\n",
            "introuvable chronopost\n",
            "chronopost renvoyer\n",
            "renvoyer balle\n",
            "\n",
            "---\n",
            "\n",
            "Cluster 7:\n",
            "renvoyer article\n",
            "article semaine\n",
            "faire annuler\n",
            "annuler commande\n",
            "rire jaune\n",
            "rappeler jour\n",
            "renvoyer encre\n",
            "encre imprimante\n",
            "telephone portable\n",
            "cloudear expedier\n",
            "expedier notice\n",
            "and difficult\n",
            "difficult assemblei\n",
            "parcours combattant\n",
            "combattant aucun\n",
            "\n",
            "---\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# PCA\n",
        "\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "pca = PCA(n_components=0.95)\n",
        "n_gram_vectors_pca = pca.fit_transform(n_gram_vectors_scaled)\n",
        "\n",
        "# Clustering avec KMeans sur les données réduites\n",
        "k = 8\n",
        "kmeans = KMeans(n_clusters=k, random_state=42)\n",
        "kmeans.fit(n_gram_vectors_pca)\n",
        "\n",
        "clusters = kmeans.labels_ # pour visualisation par entreprise plus tard\n",
        "\n",
        "cluster_n_grams = {i: [] for i in range(k)}\n",
        "for i, label in enumerate(clusters):\n",
        "    n_gram = n_gram_list[i]\n",
        "    cluster_n_grams[label].append(\" \".join(n_gram))\n",
        "\n",
        "for cluster, n_grams in cluster_n_grams.items():\n",
        "    print(f\"Cluster {cluster}:\")\n",
        "    for n_gram in n_grams[:15]:\n",
        "        print(n_gram)\n",
        "    print(\"\\n---\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "MSjF-8cfsmoB"
      },
      "outputs": [],
      "source": [
        "\n",
        "cluster_names = {\n",
        "    0: \"Problèmes de conformité\",\n",
        "    1: \"Qualité défectueuse et retours\",\n",
        "    2: \"Déception du service de livraison\",\n",
        "    3: \"Critiques sévères du service client\",\n",
        "    4: \"Délais et difficultés de communication\",\n",
        "    5: \"Déception et remboursements\",\n",
        "    6: \"Problèmes de suivi de commande\",\n",
        "    7: \"Difficultés de l'expérience d'achat\"\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I87-X-bjAr9S",
        "outputId": "db3e31e6-43ec-4492-dbda-aa016fac1034"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Score de silhouette (limité): 0.02242829462556348\n"
          ]
        }
      ],
      "source": [
        "# Score de silhouette\n",
        "\n",
        "# Calcul long d'où échantillonnage des données\n",
        "\n",
        "from sklearn.metrics import silhouette_score\n",
        "\n",
        "score = silhouette_score(n_gram_vectors_pca, kmeans.labels_, sample_size=50000)\n",
        "print(f\"Score de silhouette: {score}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "DUsJ1xL4AvWg"
      },
      "outputs": [],
      "source": [
        "# Test des clusters formés avec phrases de référence\n",
        "\n",
        "ref_df = pd.read_csv('/content/drive/My Drive/Datasets NLP/ecommerce_plaintes.csv')\n",
        "\n",
        "reference_sentences = ref_df['commentaire'].tolist()\n",
        "\n",
        "# Prétraitement des phrases\n",
        "ref_n_gram_vectors = []\n",
        "\n",
        "for sentence in reference_sentences:\n",
        "    words = text_to_word_sequence(sentence)\n",
        "    n_grams = list(ngrams(words, n))  # formation de bigrammes pour les comparer aux clusters de bigrammes\n",
        "    sentence_vectors = []\n",
        "    for n_gram in n_grams:\n",
        "        indices = [tokenizer.word_index.get(word, 0) for word in n_gram]\n",
        "        if all(idx < vocab_size for idx in indices):\n",
        "            word_vectors = embeddings[indices]\n",
        "            n_gram_vector = np.mean(word_vectors, axis=0)\n",
        "            sentence_vectors.append(n_gram_vector)\n",
        "\n",
        "    if sentence_vectors:\n",
        "        ref_n_gram_vectors.append(np.mean(sentence_vectors, axis=0))\n",
        "\n",
        "ref_n_gram_vectors = np.array(ref_n_gram_vectors)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "UyEmCNqxAxsD"
      },
      "outputs": [],
      "source": [
        "# Même PCA que pour les données d'entraînement\n",
        "\n",
        "ref_n_gram_vectors_pca = pca.transform(ref_n_gram_vectors)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 1ère méthode: Comparer les clusters attribués au jeu de test\n",
        "new_clusters = kmeans.predict(ref_n_gram_vectors_pca)\n",
        "\n",
        "for i, cluster_num in enumerate(new_clusters):\n",
        "    phrase = reference_sentences[i]\n",
        "    cluster_desc = cluster_names.get(cluster_num, f\"Cluster {cluster_num}\")\n",
        "    print(f\"Phrase: '{phrase}'\\nAttribuée au: {cluster_desc}\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KhMLueJpvJEq",
        "outputId": "4f9e1e0d-2859-4dbc-9ed8-c7e97338e3f2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Phrase: 'J'ai attendu plus d'un mois pour recevoir ma commande, c'est inacceptable.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Le suivi de mon colis indique qu'il a été livré, mais je n'ai rien reçu.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Ma commande est restée en statut 'expédiée' pendant trois semaines.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'La livraison était prévue pour hier, mais je n'ai toujours pas mon colis.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Le délai de livraison promis n'a pas été respecté, je suis très déçu.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Mon article est arrivé cassé, comment cela a-t-il pu arriver?'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Le produit reçu est endommagé, je demande un remboursement immédiat.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'J'ai reçu ma commande, mais l'article est rayé et semble utilisé.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Le colis était visiblement endommagé, et l'article à l'intérieur également.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'J'ai attendu longtemps pour ça? Un produit abîmé, super…'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Ça fait des semaines que j'attends mon remboursement, où est-il?'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Le service client m'a promis un remboursement, mais je n'ai rien reçu.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'J'ai renvoyé l'article mais toujours pas de nouvelles concernant mon remboursement'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Ils refusent de me rembourser alors que l'article est défectueux!'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Mon remboursement a été injustement refusé, je suis furieux.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'J'ai reçu un article que je n'avais pas commandé, comment est-ce possible?'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Ma commande est incomplète, il manque plusieurs articles.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Ils m'ont envoyé la mauvaise taille, je dois tout renvoyer maintenant.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Au lieu de la couleur bleue commandée, j'ai reçu un article rouge.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Il y a eu une erreur dans ma commande, et le service client ne répond pas.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Impossible de joindre le service client, leur ligne est toujours occupée.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Le service client n'a pas résolu mon problème, je suis déçu.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Après plusieurs emails, le service client n'a toujours pas répondu.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Le service client m'a raccroché au nez, incroyable!'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Le service client est incompétent, ils n'ont pas su répondre à ma question.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Ma carte a été débitée deux fois pour la même commande.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'J'ai été facturé pour des articles que je n'ai jamais reçus.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Le site a refusé mon paiement sans raison valable.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Des frais supplémentaires ont été ajoutés sans mon consentement.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Le remboursement sur ma carte de crédit prend une éternité.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Le site est tellement lent, c'est frustrant.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'J'ai eu du mal à trouver les produits que je voulais, leur moteur de recherche est mauvais.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Le site a planté juste au moment où je finalisais ma commande.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'L'interface utilisateur est peu intuitive, je me perds tout le temps.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Le site n'est pas du tout adapté aux mobiles, impossible de commander depuis mon téléphone.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Après avoir passé ma commande, on m'informe que l'article est en rupture de stock.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Ils affichent des produits disponibles qui ne le sont pas en réalité.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'J'ai commandé un article en stock, et maintenant ils disent qu'il est en rupture?'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Toujours en attente de la remise en stock promise depuis des mois.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Le manque de stock pour les articles populaires est vraiment frustrant.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Le produit reçu ne correspond pas du tout à la description sur le site.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Les images du produit étaient trompeuses, très déçu du résultat.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'L'article semblait de qualité sur le site, mais c'est tout le contraire.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'La publicité disait 'livraison gratuite', puis au paiement, surprise, des frais!'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Les caractéristiques du produit annoncées étaient exagérées, ce n'est pas ce que j'ai reçu.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'J'ai reçu des spams après m'être inscrit sur leur site, coïncidence?'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Mes données personnelles ont été partagées sans mon consentement.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Je me suis fait pirater mon compte après avoir acheté sur ce site.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Ils demandent trop d'informations personnelles pour passer commande.'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n",
            "Phrase: 'Après un achat, mon adresse email a été inondée d'offres commerciales non sollicitées'\n",
            "Attribuée au: Problèmes de suivi de commande\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7rL2hBFvAz-c",
        "outputId": "fbd76d41-0f9e-49da-e407-f9f63bb3ecc4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Phrase 'J'ai attendu plus d'un mois pour recevoir ma commande, c'est inacceptable.' est la plus proche de la problématique 'Déception du service de livraison'\n",
            "\n",
            "\n",
            "Phrase 'Le suivi de mon colis indique qu'il a été livré, mais je n'ai rien reçu.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Ma commande est restée en statut 'expédiée' pendant trois semaines.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'La livraison était prévue pour hier, mais je n'ai toujours pas mon colis.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le délai de livraison promis n'a pas été respecté, je suis très déçu.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Mon article est arrivé cassé, comment cela a-t-il pu arriver?' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le produit reçu est endommagé, je demande un remboursement immédiat.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'J'ai reçu ma commande, mais l'article est rayé et semble utilisé.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le colis était visiblement endommagé, et l'article à l'intérieur également.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'J'ai attendu longtemps pour ça? Un produit abîmé, super…' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Ça fait des semaines que j'attends mon remboursement, où est-il?' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le service client m'a promis un remboursement, mais je n'ai rien reçu.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'J'ai renvoyé l'article mais toujours pas de nouvelles concernant mon remboursement' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Ils refusent de me rembourser alors que l'article est défectueux!' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Mon remboursement a été injustement refusé, je suis furieux.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'J'ai reçu un article que je n'avais pas commandé, comment est-ce possible?' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Ma commande est incomplète, il manque plusieurs articles.' est la plus proche de la problématique 'Critiques sévères du service client'\n",
            "\n",
            "\n",
            "Phrase 'Ils m'ont envoyé la mauvaise taille, je dois tout renvoyer maintenant.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Au lieu de la couleur bleue commandée, j'ai reçu un article rouge.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Il y a eu une erreur dans ma commande, et le service client ne répond pas.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Impossible de joindre le service client, leur ligne est toujours occupée.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le service client n'a pas résolu mon problème, je suis déçu.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Après plusieurs emails, le service client n'a toujours pas répondu.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le service client m'a raccroché au nez, incroyable!' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le service client est incompétent, ils n'ont pas su répondre à ma question.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Ma carte a été débitée deux fois pour la même commande.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'J'ai été facturé pour des articles que je n'ai jamais reçus.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le site a refusé mon paiement sans raison valable.' est la plus proche de la problématique 'Difficultés de l'expérience d'achat'\n",
            "\n",
            "\n",
            "Phrase 'Des frais supplémentaires ont été ajoutés sans mon consentement.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le remboursement sur ma carte de crédit prend une éternité.' est la plus proche de la problématique 'Qualité défectueuse et retours'\n",
            "\n",
            "\n",
            "Phrase 'Le site est tellement lent, c'est frustrant.' est la plus proche de la problématique 'Difficultés de l'expérience d'achat'\n",
            "\n",
            "\n",
            "Phrase 'J'ai eu du mal à trouver les produits que je voulais, leur moteur de recherche est mauvais.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le site a planté juste au moment où je finalisais ma commande.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'L'interface utilisateur est peu intuitive, je me perds tout le temps.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le site n'est pas du tout adapté aux mobiles, impossible de commander depuis mon téléphone.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Après avoir passé ma commande, on m'informe que l'article est en rupture de stock.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Ils affichent des produits disponibles qui ne le sont pas en réalité.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'J'ai commandé un article en stock, et maintenant ils disent qu'il est en rupture?' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Toujours en attente de la remise en stock promise depuis des mois.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le manque de stock pour les articles populaires est vraiment frustrant.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Le produit reçu ne correspond pas du tout à la description sur le site.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Les images du produit étaient trompeuses, très déçu du résultat.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'L'article semblait de qualité sur le site, mais c'est tout le contraire.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'La publicité disait 'livraison gratuite', puis au paiement, surprise, des frais!' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Les caractéristiques du produit annoncées étaient exagérées, ce n'est pas ce que j'ai reçu.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'J'ai reçu des spams après m'être inscrit sur leur site, coïncidence?' est la plus proche de la problématique 'Difficultés de l'expérience d'achat'\n",
            "\n",
            "\n",
            "Phrase 'Mes données personnelles ont été partagées sans mon consentement.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Je me suis fait pirater mon compte après avoir acheté sur ce site.' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n",
            "\n",
            "Phrase 'Ils demandent trop d'informations personnelles pour passer commande.' est la plus proche de la problématique 'Critiques sévères du service client'\n",
            "\n",
            "\n",
            "Phrase 'Après un achat, mon adresse email a été inondée d'offres commerciales non sollicitées' est la plus proche de la problématique 'Problèmes de conformité'\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# 2e méthode: Calcul de similarité entre jeu de d'entraînement et de test\n",
        "\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "\n",
        "similarities = cosine_similarity(ref_n_gram_vectors_pca, kmeans.cluster_centers_)\n",
        "\n",
        "closest_clusters = np.argmax(similarities, axis=1)\n",
        "\n",
        "for i, cluster_num in enumerate(closest_clusters):\n",
        "    print(f\"\\nPhrase '{reference_sentences[i]}' est la plus proche de la problématique '{cluster_names[cluster_num]}'\\n\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "7C0oUJzRuuwq"
      },
      "outputs": [],
      "source": [
        "# Préparation des vecteurs de commentaires pour l'affichage des clusters par entreprise\n",
        "\n",
        "valid_indices = []\n",
        "comment_vectors = []\n",
        "\n",
        "# Génération des vecteurs de commentaires\n",
        "for i, comment in enumerate(df['cleaned_lemma'][df['sentiment'] == 'negatif']):\n",
        "    words = text_to_word_sequence(comment)  # Tokenisation\n",
        "    word_indices = [tokenizer.word_index.get(word, -1) for word in words]\n",
        "    valid_word_indices = [idx for idx in word_indices if 0 <= idx < len(embeddings)]\n",
        "    if valid_word_indices:\n",
        "        comment_vector = np.mean([embeddings[idx] for idx in valid_word_indices], axis=0)\n",
        "        comment_vectors.append(comment_vector)\n",
        "        valid_indices.append(i)  # Correctly storing index of the comment that contributes to comment_vectors\n",
        "\n",
        "comment_vectors = np.array(comment_vectors)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "comment_vectors_scaled = scaler.fit_transform(comment_vectors)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Association des clusters aux commentaires\n",
        "clusters = kmeans.fit_predict(comment_vectors_scaled)\n",
        "\n",
        "# Création d'un nouvea dataframe contenant uniquement les commentaires négatifs\n",
        "df_negatif_valid = df[df['sentiment'] == 'negatif'].iloc[valid_indices].copy()\n",
        "df_negatif_valid['cluster'] = clusters\n",
        "\n",
        "df_negatif_valid['cluster'] = df_negatif_valid['cluster'].replace(cluster_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acrfSpaYtoWZ",
        "outputId": "ee7066a4-86bd-4ead-d58f-12f2f9571108"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F-vvzeHzt-o_",
        "outputId": "df0b43b0-38ea-46fa-ee8f-b782cfc53066"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "company                                 Amazon  CDiscount  Fnac\n",
            "cluster                                                        \n",
            "Critiques sévères du service client       1079        846   237\n",
            "Difficultés de l'expérience d'achat        260        890   233\n",
            "Déception du service de livraison          742       1600   500\n",
            "Déception et remboursements               3017       9177  3146\n",
            "Délais et difficultés de communication     225        770    85\n",
            "Problèmes de conformité                    267       1211   279\n",
            "Problèmes de suivi de commande              27        674    12\n",
            "Qualité défectueuse et retours             534       1513   413\n"
          ]
        }
      ],
      "source": [
        "# Analyse des clusters par entreprise\n",
        "cluster_company_distribution = df_negatif_valid.groupby(['cluster', 'company']).size().unstack(fill_value=0)\n",
        "print(cluster_company_distribution)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "V100",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyO+5/tv6MLx8/f29EGXZ4Zz",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}